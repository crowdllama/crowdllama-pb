// @generated
// This file is @generated by prost-build.
/// GenerateRequest represents a request to generate a response
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateRequest {
    /// The model to use (e.g., "llama3.2")
    #[prost(string, tag="1")]
    pub model: ::prost::alloc::string::String,
    /// The input prompt
    #[prost(string, tag="2")]
    pub prompt: ::prost::alloc::string::String,
    /// Whether to stream the response
    #[prost(bool, tag="3")]
    pub stream: bool,
}
/// GenerateResponse represents a response from the generation
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateResponse {
    /// The model used
    #[prost(string, tag="1")]
    pub model: ::prost::alloc::string::String,
    /// Timestamp when response was created
    #[prost(message, optional, tag="2")]
    pub created_at: ::core::option::Option<::prost_types::Timestamp>,
    /// The generated response text
    #[prost(string, tag="3")]
    pub response: ::prost::alloc::string::String,
    /// Whether the response is complete
    #[prost(bool, tag="4")]
    pub done: bool,
    /// Reason for completion (e.g., "stop")
    #[prost(string, tag="5")]
    pub done_reason: ::prost::alloc::string::String,
    /// Token context array
    #[prost(int32, repeated, tag="6")]
    pub context: ::prost::alloc::vec::Vec<i32>,
    /// Total duration in nanoseconds
    #[prost(int64, tag="7")]
    pub total_duration: i64,
    /// Model loading duration in nanoseconds
    #[prost(int64, tag="8")]
    pub load_duration: i64,
    /// Number of prompt tokens evaluated
    #[prost(int32, tag="9")]
    pub prompt_eval_count: i32,
    /// Prompt evaluation duration in nanoseconds
    #[prost(int64, tag="10")]
    pub prompt_eval_duration: i64,
    /// Number of tokens evaluated
    #[prost(int32, tag="11")]
    pub eval_count: i32,
    /// Evaluation duration in nanoseconds
    #[prost(int64, tag="12")]
    pub eval_duration: i64,
    /// ID of the worker that processed the inference task
    #[prost(string, tag="13")]
    pub worker_id: ::prost::alloc::string::String,
}
// @@protoc_insertion_point(module)
