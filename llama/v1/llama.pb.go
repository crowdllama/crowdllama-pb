// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        (unknown)
// source: llama/v1/llama.proto

package llamav1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// BaseMessage represents a generic message that can contain either a request or response
type BaseMessage struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to Message:
	//
	//	*BaseMessage_GenerateRequest
	//	*BaseMessage_GenerateResponse
	Message       isBaseMessage_Message `protobuf_oneof:"message"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BaseMessage) Reset() {
	*x = BaseMessage{}
	mi := &file_llama_v1_llama_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BaseMessage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BaseMessage) ProtoMessage() {}

func (x *BaseMessage) ProtoReflect() protoreflect.Message {
	mi := &file_llama_v1_llama_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BaseMessage.ProtoReflect.Descriptor instead.
func (*BaseMessage) Descriptor() ([]byte, []int) {
	return file_llama_v1_llama_proto_rawDescGZIP(), []int{0}
}

func (x *BaseMessage) GetMessage() isBaseMessage_Message {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *BaseMessage) GetGenerateRequest() *GenerateRequest {
	if x != nil {
		if x, ok := x.Message.(*BaseMessage_GenerateRequest); ok {
			return x.GenerateRequest
		}
	}
	return nil
}

func (x *BaseMessage) GetGenerateResponse() *GenerateResponse {
	if x != nil {
		if x, ok := x.Message.(*BaseMessage_GenerateResponse); ok {
			return x.GenerateResponse
		}
	}
	return nil
}

type isBaseMessage_Message interface {
	isBaseMessage_Message()
}

type BaseMessage_GenerateRequest struct {
	GenerateRequest *GenerateRequest `protobuf:"bytes,1,opt,name=generate_request,json=generateRequest,proto3,oneof"`
}

type BaseMessage_GenerateResponse struct {
	GenerateResponse *GenerateResponse `protobuf:"bytes,2,opt,name=generate_response,json=generateResponse,proto3,oneof"`
}

func (*BaseMessage_GenerateRequest) isBaseMessage_Message() {}

func (*BaseMessage_GenerateResponse) isBaseMessage_Message() {}

// GenerateRequest represents a request to generate a response
type GenerateRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Model         string                 `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`    // The model to use (e.g., "llama3.2")
	Prompt        string                 `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`  // The input prompt
	Stream        bool                   `protobuf:"varint,3,opt,name=stream,proto3" json:"stream,omitempty"` // Whether to stream the response
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateRequest) Reset() {
	*x = GenerateRequest{}
	mi := &file_llama_v1_llama_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateRequest) ProtoMessage() {}

func (x *GenerateRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llama_v1_llama_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateRequest.ProtoReflect.Descriptor instead.
func (*GenerateRequest) Descriptor() ([]byte, []int) {
	return file_llama_v1_llama_proto_rawDescGZIP(), []int{1}
}

func (x *GenerateRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateRequest) GetPrompt() string {
	if x != nil {
		return x.Prompt
	}
	return ""
}

func (x *GenerateRequest) GetStream() bool {
	if x != nil {
		return x.Stream
	}
	return false
}

// GenerateResponse represents a response from the generation
type GenerateResponse struct {
	state              protoimpl.MessageState `protogen:"open.v1"`
	Model              string                 `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`                                                         // The model used
	CreatedAt          *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=created_at,json=createdAt,proto3" json:"created_at,omitempty"`                                // Timestamp when response was created
	Response           string                 `protobuf:"bytes,3,opt,name=response,proto3" json:"response,omitempty"`                                                   // The generated response text
	Done               bool                   `protobuf:"varint,4,opt,name=done,proto3" json:"done,omitempty"`                                                          // Whether the response is complete
	DoneReason         string                 `protobuf:"bytes,5,opt,name=done_reason,json=doneReason,proto3" json:"done_reason,omitempty"`                             // Reason for completion (e.g., "stop")
	Context            []int32                `protobuf:"varint,6,rep,packed,name=context,proto3" json:"context,omitempty"`                                             // Token context array
	TotalDuration      int64                  `protobuf:"varint,7,opt,name=total_duration,json=totalDuration,proto3" json:"total_duration,omitempty"`                   // Total duration in nanoseconds
	LoadDuration       int64                  `protobuf:"varint,8,opt,name=load_duration,json=loadDuration,proto3" json:"load_duration,omitempty"`                      // Model loading duration in nanoseconds
	PromptEvalCount    int32                  `protobuf:"varint,9,opt,name=prompt_eval_count,json=promptEvalCount,proto3" json:"prompt_eval_count,omitempty"`           // Number of prompt tokens evaluated
	PromptEvalDuration int64                  `protobuf:"varint,10,opt,name=prompt_eval_duration,json=promptEvalDuration,proto3" json:"prompt_eval_duration,omitempty"` // Prompt evaluation duration in nanoseconds
	EvalCount          int32                  `protobuf:"varint,11,opt,name=eval_count,json=evalCount,proto3" json:"eval_count,omitempty"`                              // Number of tokens evaluated
	EvalDuration       int64                  `protobuf:"varint,12,opt,name=eval_duration,json=evalDuration,proto3" json:"eval_duration,omitempty"`                     // Evaluation duration in nanoseconds
	WorkerId           string                 `protobuf:"bytes,13,opt,name=worker_id,json=workerId,proto3" json:"worker_id,omitempty"`                                  // ID of the worker that processed the inference task
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *GenerateResponse) Reset() {
	*x = GenerateResponse{}
	mi := &file_llama_v1_llama_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateResponse) ProtoMessage() {}

func (x *GenerateResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llama_v1_llama_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateResponse.ProtoReflect.Descriptor instead.
func (*GenerateResponse) Descriptor() ([]byte, []int) {
	return file_llama_v1_llama_proto_rawDescGZIP(), []int{2}
}

func (x *GenerateResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateResponse) GetCreatedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.CreatedAt
	}
	return nil
}

func (x *GenerateResponse) GetResponse() string {
	if x != nil {
		return x.Response
	}
	return ""
}

func (x *GenerateResponse) GetDone() bool {
	if x != nil {
		return x.Done
	}
	return false
}

func (x *GenerateResponse) GetDoneReason() string {
	if x != nil {
		return x.DoneReason
	}
	return ""
}

func (x *GenerateResponse) GetContext() []int32 {
	if x != nil {
		return x.Context
	}
	return nil
}

func (x *GenerateResponse) GetTotalDuration() int64 {
	if x != nil {
		return x.TotalDuration
	}
	return 0
}

func (x *GenerateResponse) GetLoadDuration() int64 {
	if x != nil {
		return x.LoadDuration
	}
	return 0
}

func (x *GenerateResponse) GetPromptEvalCount() int32 {
	if x != nil {
		return x.PromptEvalCount
	}
	return 0
}

func (x *GenerateResponse) GetPromptEvalDuration() int64 {
	if x != nil {
		return x.PromptEvalDuration
	}
	return 0
}

func (x *GenerateResponse) GetEvalCount() int32 {
	if x != nil {
		return x.EvalCount
	}
	return 0
}

func (x *GenerateResponse) GetEvalDuration() int64 {
	if x != nil {
		return x.EvalDuration
	}
	return 0
}

func (x *GenerateResponse) GetWorkerId() string {
	if x != nil {
		return x.WorkerId
	}
	return ""
}

var File_llama_v1_llama_proto protoreflect.FileDescriptor

const file_llama_v1_llama_proto_rawDesc = "" +
	"\n" +
	"\x14llama/v1/llama.proto\x12\bllama.v1\x1a\x1fgoogle/protobuf/timestamp.proto\"\xab\x01\n" +
	"\vBaseMessage\x12F\n" +
	"\x10generate_request\x18\x01 \x01(\v2\x19.llama.v1.GenerateRequestH\x00R\x0fgenerateRequest\x12I\n" +
	"\x11generate_response\x18\x02 \x01(\v2\x1a.llama.v1.GenerateResponseH\x00R\x10generateResponseB\t\n" +
	"\amessage\"W\n" +
	"\x0fGenerateRequest\x12\x14\n" +
	"\x05model\x18\x01 \x01(\tR\x05model\x12\x16\n" +
	"\x06prompt\x18\x02 \x01(\tR\x06prompt\x12\x16\n" +
	"\x06stream\x18\x03 \x01(\bR\x06stream\"\xd9\x03\n" +
	"\x10GenerateResponse\x12\x14\n" +
	"\x05model\x18\x01 \x01(\tR\x05model\x129\n" +
	"\n" +
	"created_at\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x1a\n" +
	"\bresponse\x18\x03 \x01(\tR\bresponse\x12\x12\n" +
	"\x04done\x18\x04 \x01(\bR\x04done\x12\x1f\n" +
	"\vdone_reason\x18\x05 \x01(\tR\n" +
	"doneReason\x12\x18\n" +
	"\acontext\x18\x06 \x03(\x05R\acontext\x12%\n" +
	"\x0etotal_duration\x18\a \x01(\x03R\rtotalDuration\x12#\n" +
	"\rload_duration\x18\b \x01(\x03R\floadDuration\x12*\n" +
	"\x11prompt_eval_count\x18\t \x01(\x05R\x0fpromptEvalCount\x120\n" +
	"\x14prompt_eval_duration\x18\n" +
	" \x01(\x03R\x12promptEvalDuration\x12\x1d\n" +
	"\n" +
	"eval_count\x18\v \x01(\x05R\tevalCount\x12#\n" +
	"\reval_duration\x18\f \x01(\x03R\fevalDuration\x12\x1b\n" +
	"\tworker_id\x18\r \x01(\tR\bworkerId2Q\n" +
	"\fLlamaService\x12A\n" +
	"\bGenerate\x12\x19.llama.v1.GenerateRequest\x1a\x1a.llama.v1.GenerateResponseB\x91\x01\n" +
	"\fcom.llama.v1B\n" +
	"LlamaProtoP\x01Z4github.com/crowdllama/crowdllama-pb/llama/v1;llamav1\xa2\x02\x03LXX\xaa\x02\bLlama.V1\xca\x02\bLlama\\V1\xe2\x02\x14Llama\\V1\\GPBMetadata\xea\x02\tLlama::V1b\x06proto3"

var (
	file_llama_v1_llama_proto_rawDescOnce sync.Once
	file_llama_v1_llama_proto_rawDescData []byte
)

func file_llama_v1_llama_proto_rawDescGZIP() []byte {
	file_llama_v1_llama_proto_rawDescOnce.Do(func() {
		file_llama_v1_llama_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_llama_v1_llama_proto_rawDesc), len(file_llama_v1_llama_proto_rawDesc)))
	})
	return file_llama_v1_llama_proto_rawDescData
}

var file_llama_v1_llama_proto_msgTypes = make([]protoimpl.MessageInfo, 3)
var file_llama_v1_llama_proto_goTypes = []any{
	(*BaseMessage)(nil),           // 0: llama.v1.BaseMessage
	(*GenerateRequest)(nil),       // 1: llama.v1.GenerateRequest
	(*GenerateResponse)(nil),      // 2: llama.v1.GenerateResponse
	(*timestamppb.Timestamp)(nil), // 3: google.protobuf.Timestamp
}
var file_llama_v1_llama_proto_depIdxs = []int32{
	1, // 0: llama.v1.BaseMessage.generate_request:type_name -> llama.v1.GenerateRequest
	2, // 1: llama.v1.BaseMessage.generate_response:type_name -> llama.v1.GenerateResponse
	3, // 2: llama.v1.GenerateResponse.created_at:type_name -> google.protobuf.Timestamp
	1, // 3: llama.v1.LlamaService.Generate:input_type -> llama.v1.GenerateRequest
	2, // 4: llama.v1.LlamaService.Generate:output_type -> llama.v1.GenerateResponse
	4, // [4:5] is the sub-list for method output_type
	3, // [3:4] is the sub-list for method input_type
	3, // [3:3] is the sub-list for extension type_name
	3, // [3:3] is the sub-list for extension extendee
	0, // [0:3] is the sub-list for field type_name
}

func init() { file_llama_v1_llama_proto_init() }
func file_llama_v1_llama_proto_init() {
	if File_llama_v1_llama_proto != nil {
		return
	}
	file_llama_v1_llama_proto_msgTypes[0].OneofWrappers = []any{
		(*BaseMessage_GenerateRequest)(nil),
		(*BaseMessage_GenerateResponse)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_llama_v1_llama_proto_rawDesc), len(file_llama_v1_llama_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   3,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_llama_v1_llama_proto_goTypes,
		DependencyIndexes: file_llama_v1_llama_proto_depIdxs,
		MessageInfos:      file_llama_v1_llama_proto_msgTypes,
	}.Build()
	File_llama_v1_llama_proto = out.File
	file_llama_v1_llama_proto_goTypes = nil
	file_llama_v1_llama_proto_depIdxs = nil
}
